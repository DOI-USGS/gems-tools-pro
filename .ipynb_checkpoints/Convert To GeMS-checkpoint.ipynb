{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting published digital geologic map data into GeMS\n",
    "#### Notes on converting the generalized layers of the Alaska state geologic map, SIM 3340, from the 'NSA' schema to GeMS\n",
    "\n",
    "This notebook documents the process of converting the generalized version of the Alaska state geologic map, published in USGS SIM 3340, from the published schema, aka NSA, into GeMS. Note, however that the generalized layers of the map, **AKStategeolpoly_generalized** and **AKStategeolarc_generalized**, stored as feature classes within the geodatabase **AKStategeol.gdb**, were simplified from more richly attributed feature classes and therefore present an easier task of conversion than converting those detailed feature classes.\n",
    "\n",
    "This notebook also serves as a tutorial for using tools from the GeMS Tools Python toolbox as well as standard Anaconda modules for data science inside a Jupyter Notebook. One advantage to doing this is that  markdown notes are saved alongside tool input parameters and other python code used to explore and clean data during the conversion.\n",
    "\n",
    "To add this Notebook to an ArcGIS Pro project either\n",
    "1. On the Insert tab, click the drop-down next to **New Notebook** and browse to this file, or\n",
    "2. In the Catalog pane, right-click on **Notebooks** on the **Project** tab and browse to this file.\n",
    "\n",
    "To open the Notebook, find the notebook under **Notebooks** and double-click or right-click and choose **Open Notebook**\n",
    "\n",
    "This notebook uses a version of the GeMS toolbox which is under development. You can download it from the ```v3.0``` branch of the ```gems-tools-pro``` repo (https://github.com/usgs/gems-tools-pro/tree/v3.0) and use the location of the ```.pyt``` file as the parameter of the ```ImportToolbox``` in section 4.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import arcpy\n",
    "import inspect\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bool(param):\n",
    "    '''Return a boolean for various possibilities of boolean-like values'''\n",
    "    return_bool = False\n",
    "    if param in [1, True, '1', 'yes', 'Yes', 'true', 'True', 'on', 'On', 'hella']:\n",
    "        return_bool = True\n",
    " \n",
    "    return return_bool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Explore the original data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1  Set variables to layers in the map\n",
    "Explore the published feature classes. In the case of this Notebook, I am using it inside of an ArcGIS Pro project which also has a map which contains the two feature classes I will be converting. Since everything is contained in the same project, to get a handle on the layers, I only have to refer to them by name, I don't have to worry about paths. The layer names, made from the feature class names in the original database are a little long so I will assign the names to shorter variables. Just make sure that the feature classes in the map do not have any selections or definition filters on them as those will be honored by any geoprocessing we run here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_lines = \"AKStategeolarc_generalized\"\n",
    "gen_polys = \"AKStategeolpoly_generalized\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Properties of the lines \n",
    "I will start with the lines. How many are there in this feature class? Let's compare a couple methods to determine this in terms of processing time. First, there is the ArcGIS tool, **Get Count**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8353\n",
      "Wall time: 261 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# use cell magic command %% time to time the execution in a cell, use % time to time a line\n",
    "print(arcpy.GetCount_management(gen_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, using pandas. First, use the ArcGIS tool TableToNumPyArray and then make a pandas dataframe out of that. To get the number of rows in a dataframe use ```df.shape[0]``` or ```len(df.index)```. Both are faster than GetCount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 102 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8353"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "arr = arcpy.da.TableToNumPyArray(gen_lines, 'OBJECTID') # only need one field to check the row count\n",
    "df = pd.DataFrame(arr)\n",
    "len(df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a list of the field names. Remember that ```arcpy.ListFields``` returns field objects which have many properties, including ```.name```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OBJECTID', 'SHAPE', 'ARC_CODE', 'LINE_TYPE', 'SHAPE_Length']\n"
     ]
    }
   ],
   "source": [
    "field_objects = arcpy.ListFields(gen_lines)\n",
    "field_names = [f.name for f in field_objects]\n",
    "print(field_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a list of the unique pairs of ```ARC_CODE``` and ```LINE_TYPE``` to make sure those are all consistent.\n",
    "I could use the Frequency tool in ArcGIS, but I think pandas will do a better job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ARC_CODE                                          LINE_TYPE  count\n",
      "0          4                   Normal fault, location certain\\r    270\n",
      "1          5               Normal fault, location approximate\\r    282\n",
      "2          6           Normal fault, location inferred, queried     33\n",
      "3         10  Thrust fault, location certain, teeth on right...   1706\n",
      "4         11  Thrust fault, location approximate, teeth on r...    842\n",
      "5         12  Thrust fault, location inferred, queried, teet...    229\n",
      "6         14                            Caldera or crater rim\\r    123\n",
      "7         28         Caldera or crater rim, inferred, concealed     33\n",
      "8         30  Fault, sense of displacement uncertain, locati...    616\n",
      "9         31  Fault, sense of displacement uncertain, locati...    300\n",
      "10        32  Fault, sense of displacement uncertain, locati...     43\n",
      "11        35  High-angle reverse fault, location certain, te...     19\n",
      "12        36  High-angle reverse fault, location approximate...     49\n",
      "13        37  High-angle reverse fault, location inferred, t...      1\n",
      "14        52                             Concealed normal fault    809\n",
      "15        53  Concealed thrust fault, teeth on right from or...   1771\n",
      "16        54  Concealed high-angle reverse fault, teeth on r...     45\n",
      "17        55  Concealed normal fault, having right lateral o...     34\n",
      "18        57                      Concealed right-lateral fault    283\n",
      "19        58                       Concealed left-lateral fault     54\n",
      "20        60          Concealed fault of uncertain displacement    448\n",
      "21        71  Normal fault, location certain, having right l...     35\n",
      "22        72  Normal fault, location approx., having right l...      2\n",
      "23        76  Normal fault, location inferred, queried, havi...      5\n",
      "24        87            Right lateral fault, location certain\\r    177\n",
      "25        88        Right lateral fault, location approximate\\r     70\n",
      "26        89    Right lateral fault, location inferred, queried     25\n",
      "27        90               Left lateral fault, location certain     28\n",
      "28        91           Left lateral fault, location approximate     21\n"
     ]
    }
   ],
   "source": [
    "arr = arcpy.da.TableToNumPyArray(gen_lines, ('ARC_CODE', 'LINE_TYPE'))\n",
    "df = pd.DataFrame(arr)\n",
    "uniq = df.groupby(['ARC_CODE', 'LINE_TYPE']).size().reset_index().rename(columns={0:'count'})\n",
    "pd.options.display.width = 0  # this should tell pandas to expand the columns as necessary to display the entire value,\n",
    "# otherwise, long LINE_TYPES will likely be truncated with ellipses, but could also use pd.set_option('display.width', 100)\n",
    "print(uniq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some things to note here about the results to keep in mind for later, after the data have been loaded into the GeMS database:\n",
    "1. There are a few line types that have very few occurrences. Are these possibly mistakes in identification? Is it important to retain these values in a state-wide generalization?\n",
    "2. There are some values that end in non-printing, end-of-line characters '\\r'. These should be removed at some point.\n",
    "3. Lines marked 'Caldera rim' should be separated into the GeologicLines feature class defined by the GeMS schema. ContactsAndFaults is for lines that model the surface expressions of the 3D planes that separate rock units and participate in a topologic relationship with MapUnitPolys. Lines that model geomorphology and that are not coincident with map unit polygon boundaries are a different concept than contacts and belong in a different feature class.\n",
    "4. And the big one is that there are no topologic constraints between the lines and the polygons. The metadata for **AKStategeolarc_generalized** says it *\"is a subset of AKStategeol_arc, consisting of caldera rims and major faults.\"* and the generalized polygons were created by a process of merging, rasterization, raster region smoothing, and then vectorization back to polygons. They were not created from the lines in **AKStategeolarc_generalized**. Thus, the lines are not coincident with polygon bounaries and could not be used to rebuild **MapUnitPolys** if there was to be a change. Not having valid topologic relationships between **ContactsAndFaults** and **MapUnitPolys** breaks Level 2 and 3 GeMS compliance. Not sure what to do about this.\n",
    "\n",
    "As far as conversion to GeMS goes, the ```LINE_TYPE``` values will need to be parsed into the various GeMS fields. We will use the Attribute By Key Values tool for that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Properties of the polygons\n",
    "Again, lets do some simple data exploration to get an idea of what to look out for later in the conversion.\n",
    "\n",
    "First, a count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 154 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15466"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "arr = arcpy.da.TableToNumPyArray(gen_polys, 'OBJECTID') # only need one field to check the row count\n",
    "df = pd.DataFrame(arr)\n",
    "len(df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list of the fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OBJECTID', 'Shape', 'GROUP_ID', 'GROUP_SYMBOL', 'GROUP_LABEL', 'GROUP_LABEL2', 'GROUP_NAME', 'GROUP_AGE', 'Shape_Length', 'Shape_Area']\n"
     ]
    }
   ],
   "source": [
    "field_objects = arcpy.ListFields(gen_polys)\n",
    "field_names = [f.name for f in field_objects]\n",
    "print(field_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the unique groups of values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    GROUP_LABEL2  GROUP_SYMBOL  count\n",
      "0          CDbrv           303     19\n",
      "1           CPxt           245     15\n",
      "2          CPxwg           456     21\n",
      "3          CPxwn           255    129\n",
      "4             Ca           164      9\n",
      "5            Clg           610    123\n",
      "6          Clgne           533     33\n",
      "7          Clgtk           533     45\n",
      "8            Crc           624      6\n",
      "9           DCbg           924     60\n",
      "10          DCmt           895      6\n",
      "11          DCsp            45     97\n",
      "12         DCwbl           535     60\n",
      "13          DOgi           147     45\n",
      "14          DOls           505      4\n",
      "15          DOsc           505    146\n",
      "16          DOtu           805      6\n",
      "17         DPxcn           797     40\n",
      "18         DPxga           265     13\n",
      "19         DPxnl           406     13\n",
      "20         DPxsb           496     81\n",
      "21         DPxsq            65     96\n",
      "22          DSsm           714    131\n",
      "23          DSum           609      2\n",
      "24           DSv           316     21\n",
      "25          DZwp           514     42\n",
      "26          DZyf           805    139\n",
      "27          DZyk           805      7\n",
      "28           Das           125     34\n",
      "29           Dbf           606    105\n",
      "30           Dcc           527     44\n",
      "31          Degh           832    121\n",
      "32          Degn           821     80\n",
      "33           Dgb           815     18\n",
      "34          Dvec           428     11\n",
      "35         IPDcf           825      8\n",
      "36          IPMn           823      7\n",
      "37          IPsb           823      1\n",
      "38          JDmc           522     16\n",
      "39          JDoc           658    211\n",
      "40          JIPe           850    152\n",
      "41          JMct           842     12\n",
      "42          JMps           874     49\n",
      "43           JPk           892     53\n",
      "44           JPs           612     17\n",
      "45          JPzc           733      6\n",
      "46          JPzs           843      3\n",
      "47         JTrkp           960      5\n",
      "48         JTrmv           556     28\n",
      "49         JTros           717     26\n",
      "50        JTrsch           403     19\n",
      "51         JTrsr           940     17\n",
      "52           JZu           708     95\n",
      "53           Jag           228      2\n",
      "54          Jegr           379     22\n",
      "55            Jk           640     41\n",
      "56         Jlmgr           258     95\n",
      "57          Jsct           793     20\n",
      "58           Jtk           407     74\n",
      "59           KDt           853     19\n",
      "60          KJab           348    159\n",
      "61          KJgn           541    188\n",
      "62          KJgu           558     58\n",
      "63           KJs           670      7\n",
      "64          KJse           248     96\n",
      "65         KJsnk           670     91\n",
      "66          KJvp           107     48\n",
      "67          KJyg           571     29\n",
      "68          KJyh           972      2\n",
      "69           KMm           633     43\n",
      "70          KPss           934     29\n",
      "71         KPzum           448    115\n",
      "72          KTrm           375     41\n",
      "73         KTrvs           942     49\n",
      "74          Kcca           341     27\n",
      "75          Kcgc           990    137\n",
      "76          Kchf           583    343\n",
      "77          Kcvg           591      5\n",
      "78           Keg           338     89\n",
      "79           Kfy           484    111\n",
      "80           Khs           860      8\n",
      "81          Kipc           582     48\n",
      "82            Kk           562    264\n",
      "83           Kke           662      6\n",
      "84           Kkg           541     72\n",
      "85          Klgr            78     84\n",
      "86          Kmgr            58    318\n",
      "87          Kmig           139     16\n",
      "88           Kms           109      4\n",
      "89          Kmss           690     10\n",
      "90          Kmuc           425     62\n",
      "91          Knmt           350    278\n",
      "92           Kns           540    146\n",
      "93           Kok           664    106\n",
      "94           Kpf           570      3\n",
      "95           Kps           990      1\n",
      "96           Ksb           443     40\n",
      "97          Ksbf           653     37\n",
      "98          Ksfg           187     29\n",
      "99          Ksmd           840    101\n",
      "100         Kvgc           894     18\n",
      "101          Kvu            69     27\n",
      "102          Kyg           854     28\n",
      "103          MDe           904     74\n",
      "104        MDegk           945    128\n",
      "105         MDip           925      6\n",
      "106         MDmg           216    116\n",
      "107         MDts           813     23\n",
      "108          MDv           856      7\n",
      "109         MOkg           815     21\n",
      "110        MPxgs           386    111\n",
      "111          Mgq           925      9\n",
      "112           Mk           926    145\n",
      "113        Mlgac           412     60\n",
      "114         Mlgk           621     41\n",
      "115        Mlgnu           533     33\n",
      "116        MzPza           554      1\n",
      "117       MzPzcp           562    145\n",
      "118       MzPzka           883      1\n",
      "119       MzPzmb           547     16\n",
      "120          Mzm           243     42\n",
      "121         OCjr           278     12\n",
      "122          OCv           539     17\n",
      "123        OPxpt           144      5\n",
      "124           Oc           104     25\n",
      "125         PDcf           704     54\n",
      "126         PDms           552     29\n",
      "127        PIPgi           208     29\n",
      "128        PIPsm           825     77\n",
      "129          Pcs           520     20\n",
      "130         Plss           732     22\n",
      "131         Pstc           267     44\n",
      "132           Pv           209      1\n",
      "133         Pxkd           356     12\n",
      "134         Pxqm           175     12\n",
      "135          Pxv           468     13\n",
      "136       PzPxgb           698      1\n",
      "137       PzPxkg           186     25\n",
      "138       PzPxnc           654     48\n",
      "139      PzPxrqm           378     66\n",
      "140       PzZncl           707     63\n",
      "141          Pzc           518     83\n",
      "142         Pzcu           412      4\n",
      "143          Pze           565      1\n",
      "144         Pzkn           612     65\n",
      "145         Pzls           803     13\n",
      "146         Pznp           875      2\n",
      "147         QTgm           172     34\n",
      "148          QTm           293      3\n",
      "149          QTs            20   3322\n",
      "150         QTvi             4    360\n",
      "151         QTvs            12     16\n",
      "152         SCda           736      6\n",
      "153        SCwbc           528     19\n",
      "154         SOig           508      2\n",
      "155         SOmi           336     18\n",
      "156         SOpw           508     60\n",
      "157        SZfwr           186     29\n",
      "158         TKcf           422     36\n",
      "159         TKgi            28    338\n",
      "160         TKgr           123     11\n",
      "161         TKkf           353      1\n",
      "162          TKm             9     14\n",
      "163         TKpc           274     53\n",
      "164         TKpr            24    183\n",
      "165          TKs           286     24\n",
      "166        TKtsp           459     92\n",
      "167        TMzmb           242     26\n",
      "168         TMzu           574      7\n",
      "169         TPzi           499     29\n",
      "170          Tbk           246      3\n",
      "171          Tcb           142     57\n",
      "172          Tcc           152      2\n",
      "173          Tcl           362     11\n",
      "174          Tcp           108     62\n",
      "175         Tcpp            55     72\n",
      "176         Tehi            67      7\n",
      "177          Tgb           407     34\n",
      "178          Thi           205     42\n",
      "179           Tk           283     14\n",
      "180         Tknt           392    169\n",
      "181          Tmi            35     81\n",
      "182          Tng           283     33\n",
      "183         Toeg            39    186\n",
      "184        Togum           446      9\n",
      "185         Togv           345     30\n",
      "186          Top           383     27\n",
      "187         Tovs           474    110\n",
      "188         Tpgi            99     38\n",
      "189        TrDtz           615    122\n",
      "190       TrIPms           803     74\n",
      "191       TrIPsf           732      3\n",
      "192        TrMsm           624     16\n",
      "193        TrPsg           884     59\n",
      "194        TrPvs           775     14\n",
      "195       TrPzbi           766     17\n",
      "196       TrPzgp           766     44\n",
      "197         Trcs           941     23\n",
      "198         Trgs           992     23\n",
      "199         Trhg           877     53\n",
      "200         Trmb           439    126\n",
      "201        Trmls           970     44\n",
      "202         Trqd           128     26\n",
      "203         Trsf           440     13\n",
      "204          Tsf           383     22\n",
      "205         Tski           274     14\n",
      "206         Tsmo           365     80\n",
      "207          Tsu            93     22\n",
      "208         Ttsr           286     18\n",
      "209           Tv           117    158\n",
      "210          Tvc           184      2\n",
      "211         Tvcs           272     53\n",
      "212         Tvme           314    227\n",
      "213         Tvpm           224     51\n",
      "214          Xio           598     15\n",
      "215          Zam           264      3\n",
      "216          Zgn           298      8\n",
      "217           bu           978    138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218            g             0    413\n"
     ]
    }
   ],
   "source": [
    "fields = ['GROUP_ID', 'GROUP_SYMBOL', 'GROUP_LABEL', 'GROUP_LABEL2', 'GROUP_NAME', 'GROUP_AGE']\n",
    "arr = arcpy.da.TableToNumPyArray(gen_polys, fields)\n",
    "df = pd.DataFrame(arr)\n",
    "uniq = df.groupby(['GROUP_LABEL2', 'GROUP_SYMBOL']).size().reset_index().rename(columns={0:'count'})\n",
    "pd.set_option('display.max_rows', 220) \n",
    "print(uniq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, there are some classifications that only apply to one or a small handful of features, which seems odd. Keep that in mind. As far as converting to GeMS goes, there are fields that will map directly to GeMS fields, eg\n",
    "\n",
    "| NSA          | GeMS                     |\n",
    "|--------------|--------------------------|\n",
    "| GROUP_LABEL2 | MapUnit                  |\n",
    "| GROUP_LABEL  | Label                    |\n",
    "| GROUP_SYMBOL | Symbol (maps to wpgcmyk) |\n",
    "| GROUP_NAME   | Fullname (in DMU table)  |\n",
    "| GROUP_AGE    | Age (in DMU table)       |\n",
    "\n",
    "An obvious value missing is ```DataSource```. For this generalized version of the state geologic map, Ric Wilson did not put data sources into the feature classes, nor are there relationship classes with any of the database tables. To get data sources for the ```GROUP```s here, one has to go through the ```genstate``` table which links ```GROUP_ID``` (or ```GROUP_LABEL```) to ```State_label``` which can then be linked through ```AKstategeol_poly``` to a ```SOURCE``` which can be looked up in ```nsarefs```.  (I don't believe there is a normalized lookup table to link these two values). The simplest solution for the GeMS version is to just use 'SIM3340'. Another way would be to try to collate the data sources for all of the NSACLASS units that were grouped together into a group but to do this would require also publishing some of the database tables and defining relationships. If I can build GeMS tools to recognize pipe-delimited ```DataSourceID```s, this could work.\n",
    "\n",
    "Deciding what put in the ```Description``` field of a GeMS DMU is a similar problem. The published generalized map plate has unit names but no descriptions. The NSA units on the generalized map that are the same as in the database have descriptions in ```genstate```, but those that are comprised of other units and don't have a single description describing the generalized unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0 Transcribe original data to GeMS geodatabase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Import the GeMS Tools toolbox\n",
    "The GeMS tools can be called as standalone scripts, but if the Python toolbox is imported instead, it is more like importing a python module where individual tools are referenced as classes, and we get some of the internal validation that is performed before the tool is run.\n",
    "\n",
    "Use ```arcpy.ImportToolbox``` and provide the full path to the .pyt file (version 3.0 or higher of the ArcGIS Pro version of the toolbox.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'gems'>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arcpy.ImportToolbox(r\"C:\\_AAA\\gems\\gitspace\\gems-tools-pro\\gems-toolbox\\GeMS Tools.pyt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The alias for the toolbox is ```gems```. The tools are now available by using ```arcpy.<GeMStoolname>_gems```. You may notice this is the same syntax as for many other ```arcpy``` tools such as ```arcpy.TableToTable_conversion``` or ```arcpy.Delete_management```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Create an empty GeMS database \n",
    "Let's create an empty GeMS database into which the original data will be loaded. We will use the ```CreateDatabase``` tool which is in the ```Create and Edit``` toolset in the GeMS toolbox. You can find help on using the tool by opening the tool parameter form from the Catalog pane and reading the hover tips, by reading the <a href=\"https://github.com/usgs/gems-tools-pro/wiki/GeMS_ToolsDocumentation#CreateNewDatabase\" target=\"_blank\">Wiki entry</a>), or by calling the tool here and not providing any parameters (in this case, ignore the ExecuteError messages block and I don't know why the Usage information is sometimes printed twice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage:\n",
      "       GeMS_CreateDatabase_Arc10.1.py [directory] [geodatabaseName] [coordSystem]\n",
      "                    [OptionalElements] [#XSections] [AddEditTracking] [AddRepresentations] [AddLTYPE]\n",
      "       [directory] Name of a directory. Must exist and be writable.\n",
      "       [geodatabaseName] Name of gdb to be created, with or without .gdb extension.\n",
      "       [coordSystem] May select an ESRI projection file, import a spatial reference from an existing dataset, or \n",
      "          define a new spatial reference system from scratch.\n",
      "       [OptionalElements] List of optional feature classes to add to GDB, e.g.,\n",
      "          [OrientationPoints, CartographicLines, RepurposedSymbols]. May be empty.\n",
      "       [#XSections] is an integer (0, 1, 2, ...) specifying the intended number of\n",
      "          cross-sections\n",
      "       [AddEditTracking] Enables edit tracking on all feature classes. Adds fields created_user, \n",
      "          created_date, last_edited_user, and last_edited_date. Dates are recorded in database (local) time. \n",
      "          Default is checked. This parameter is ignored if the installed version of ArcGIS is less than 10.1.\n",
      "       [AddLTYPE] If true, add LTYPE field to feature classes ContactsAndFaults and GeologicLines, add PTTYPE field\n",
      "          to feature class OrientationData, and add PTTYPE field to MapUnitLabelPoints \n",
      "       [AddCONF] If true: 1) Attaches standard values of \"certain\" and \"questionable\" as a coded-value domain to \n",
      "          all ExistenceConfidence, IdentityConfidence, and ScientificConfidence fields. 2) Adds definitions and \n",
      "          definition source for \"certain\" and \"questionable\" to the Glossary table. 3) Adds the definition source \n",
      "          (FGDC-STD-013-2006) to the DataSources table.\n",
      "\n",
      "        Boolean parameters may be set with 1, \"1\", \"0\" \"true\", \"false\", True, \"yes\", \"Yes\",  \n",
      "    \n",
      "Usage:\n",
      "       GeMS_CreateDatabase_Arc10.1.py [directory] [geodatabaseName] [coordSystem]\n",
      "                    [OptionalElements] [#XSections] [AddEditTracking] [AddRepresentations] [AddLTYPE]\n",
      "       [directory] Name of a directory. Must exist and be writable.\n",
      "       [geodatabaseName] Name of gdb to be created, with or without .gdb extension.\n",
      "       [coordSystem] May select an ESRI projection file, import a spatial reference from an existing dataset, or \n",
      "          define a new spatial reference system from scratch.\n",
      "       [OptionalElements] List of optional feature classes to add to GDB, e.g.,\n",
      "          [OrientationPoints, CartographicLines, RepurposedSymbols]. May be empty.\n",
      "       [#XSections] is an integer (0, 1, 2, ...) specifying the intended number of\n",
      "          cross-sections\n",
      "       [AddEditTracking] Enables edit tracking on all feature classes. Adds fields created_user, \n",
      "          created_date, last_edited_user, and last_edited_date. Dates are recorded in database (local) time. \n",
      "          Default is checked. This parameter is ignored if the installed version of ArcGIS is less than 10.1.\n",
      "       [AddLTYPE] If true, add LTYPE field to feature classes ContactsAndFaults and GeologicLines, add PTTYPE field\n",
      "          to feature class OrientationData, and add PTTYPE field to MapUnitLabelPoints \n",
      "       [AddCONF] If true: 1) Attaches standard values of \"certain\" and \"questionable\" as a coded-value domain to \n",
      "          all ExistenceConfidence, IdentityConfidence, and ScientificConfidence fields. 2) Adds definitions and \n",
      "          definition source for \"certain\" and \"questionable\" to the Glossary table. 3) Adds the definition source \n",
      "          (FGDC-STD-013-2006) to the DataSources table.\n",
      "\n",
      "        Boolean parameters may be set with 1, \"1\", \"0\" \"true\", \"false\", True, \"yes\", \"Yes\",  \n",
      "    \n"
     ]
    },
    {
     "ename": "ExecuteError",
     "evalue": "Failed to execute. Parameters are not valid.\nERROR 000735: Output Workspace: Value is required\nERROR 000735: Name of new geodatabase: Value is required\nERROR 000735: Spatial reference system: Value is required\nFailed to execute (CreateDatabase).\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mExecuteError\u001b[0m                              Traceback (most recent call last)",
      "In  \u001b[0;34m[37]\u001b[0m:\nLine \u001b[0;34m1\u001b[0m:     arcpy.CreateDatabase_gems()\n",
      "File \u001b[0;34mC:\\_AAA\\gems\\gitspace\\gems-tools-pro\\gems-toolbox\\GeMS Tools.pyt\u001b[0m, in \u001b[0;32mCreateDatabase\u001b[0m:\nLine \u001b[0;34m282\u001b[0m:   \u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mDeplanarize\u001b[39;49;00m(\u001b[36mobject\u001b[39;49;00m):\n",
      "File \u001b[0;34mC:\\_AAA\\gems\\gitspace\\gems-tools-pro\\gems-toolbox\\GeMS Tools.pyt\u001b[0m, in \u001b[0;32mCreateDatabase\u001b[0m:\nLine \u001b[0;34m279\u001b[0m:   \u001b[37m# the script tool has been imported as a module. Now call the main function\u001b[39;49;00m\n",
      "File \u001b[0;34mC:\\ArcGIS\\Pro\\Resources\\ArcPy\\arcpy\\geoprocessing\\_base.py\u001b[0m, in \u001b[0;32m<lambda>\u001b[0m:\nLine \u001b[0;34m512\u001b[0m:   \u001b[34mreturn\u001b[39;49;00m \u001b[34mlambda\u001b[39;49;00m *args: val(*gp_fixargs(args, \u001b[34mTrue\u001b[39;49;00m))\n",
      "\u001b[0;31mExecuteError\u001b[0m: Failed to execute. Parameters are not valid.\nERROR 000735: Output Workspace: Value is required\nERROR 000735: Name of new geodatabase: Value is required\nERROR 000735: Spatial reference system: Value is required\nFailed to execute (CreateDatabase).\n\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "arcpy.CreateDatabase_gems()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, prepare the arguments. Note that ```crs``` can be the name of another feature class with coordinates in the spatial reference you want to use. Outside of an ArcGIS Pro you would have to supply the full path to the feature class, but if you are using this code inside of an ArcGIS Pro project and the layer has been added to a map there, just call the name.\n",
    "\n",
    "Because of the lines marked 'Caldera rim', we'll add a **GeologicLines** feature class. The full list of optional feature classes is:\n",
    "\n",
    "```'CartographicLines', 'CorrelationOfMapUnits', 'DataSourcePolys', 'FossilPoints', 'GenericPoints', 'GeochronPoints', 'GeologicLines', 'IsoValueLines', 'MapUnitLines', 'MapUnitPoints', 'MapUnitOverlayPolys', 'MiscellaneousMapInformation', 'OrientationPoints', 'OverlayPolys', 'RepurposedSymbols', 'StandardLithology', 'Stations'```\n",
    "\n",
    "Copy the classes needed from here and put them in the ```optional``` list below.\n",
    "\n",
    "Because we will be parsing the ```LINE_TYPE``` into a few fields, set the ```addLtype``` parameter to true. When the data are loaded, we'll save ```LINE_TYPE``` values to ```LTYPE``` and then use the Attribute By Key Values tool to parse the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = r\"C:\\_AAA\\mrp\\statemap\\nsa2gems\"\n",
    "gdb_name = 'Alaska_generalized'\n",
    "#crs = 'PROJCS[\"NAD_1983_Alaska_Albers\",GEOGCS[\"GCS_North_American_1983\",DATUM[\"D_North_American_1983\",SPHEROID[\"GRS_1980\",6378137.0,298.257222101]],PRIMEM[\"Greenwich\",0.0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Albers\"],PARAMETER[\"False_Easting\",0.0],PARAMETER[\"False_Northing\",0.0],PARAMETER[\"Central_Meridian\",-154.0],PARAMETER[\"Standard_Parallel_1\",55.0],PARAMETER[\"Standard_Parallel_2\",65.0],PARAMETER[\"Latitude_Of_Origin\",50.0],UNIT[\"Meter\",1.0]]\n",
    "# for coordinate system you can supply the path to a .prj file (or the text contents of one, above) or the name of another feature class that is in the coordinate system you want to use\n",
    "crs = \"AKStategeolpoly_generalized\"\n",
    "optional = ['GeologicLines']\n",
    "xsections = 0\n",
    "addTrack = False\n",
    "addLtype = True\n",
    "addConfs = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And call the tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gdb_path = os.path.join(outdir, gdb_name+'.gdb')\n",
    "if arcpy.Exists(gdb_path):\n",
    "    arcpy.Delete_management(gdb_path)\n",
    "    \n",
    "arcpy.CreateDatabase_gems(outdir, gdb_name, crs, optional, xsections, addTrack, addLtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Load original data\n",
    "When you load data in the **Catalog** pane by right-clicking on a feature class and selecting **Load Data**, it is the <a href=\"https://pro.arcgis.com/en/pro-app/latest/tool-reference/data-management/append.htm\" target=\"_blank\">Append</a> tool for which you fill out a parameter form. In the cell below, I have defined the parameters for using the tool in arcpy. To get these, I first ran the tool after filling out the parameter form and then used the **Copy Python Command** from the **History** pane (right-click on the history item). I then parsed the parameters to the variables below. Again, running some of the tools that are in this Notebook using their parameter forms will be easier than running them from the Notebook, but the point here is to document the process. \n",
    "\n",
    "Note: \n",
    "1. you can also create a <a href=\"https://pro.arcgis.com/en/pro-app/latest/arcpy/get-started/mapping-fields.htm\" target=\"_blank\">```FieldMappings```</a> object using arcpy,\n",
    "2. you may have to review some SQL to write a valid expression if you need to filter features. But remember that you can apply definition queries or selections to your input feature class in the map and they will be honored while in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_fc = gen_lines\n",
    "target = r\"C:\\_AAA\\mrp\\statemap\\nsa2gems\\Alaska_generalized.gdb\\GeologicMap\\ContactsAndFaults\"\n",
    "field_mapping_type = \"NO_TEST\"\n",
    "# below is what the field mapping parameter looks like when I copy the entire python command generated by the parameter form\n",
    "# (with inserted line breaks for readability)\n",
    "\n",
    "# '''\n",
    "# Type \"Type\" true true false 254 Text 0 0,First,#;\n",
    "# IsConcealed \"IsConcealed\" true true false 1 Text 0 0,First,#;\n",
    "# LocationConfidenceMeters \"LocationConfidenceMeters\" true true false 4 Float 0 0,First,#;\n",
    "# ExistenceConfidence \"ExistenceConfidence\" true true false 50 Text 0 0,First,#;\n",
    "# IdentityConfidence \"IdentityConfidence\" true true false 50 Text 0 0,First,#;\n",
    "# Label \"Label\" true true false 50 Text 0 0,First,#;\n",
    "# Symbol \"Symbol\" true true false 254 Text 0 0,First,#;\n",
    "# DataSourceID \"DataSourceID\" true true false 50 Text 0 0,First,#;\n",
    "# Notes \"Notes\" true true false 254 Text 0 0,First,#;\n",
    "# ContactsAndFaults_ID \"ContactsAndFaults_ID\" true true false 50 Text 0 0,First,#;\n",
    "# LTYPE \"LTYPE\" true true false 200 Text 0 0,First,#,AKStategeolarc_generalized,LINE_TYPE,0,100\n",
    "# '''\n",
    "\n",
    "# but I only have to include the field map objects for fields that will store transcribed values.\n",
    "field_mapping = '''\n",
    "LTYPE \"LTYPE\" true true false 200 Text 0 0,First,#,AKStategeolarc_generalized,LINE_TYPE,0,100\n",
    "'''\n",
    "subtype = \"\"\n",
    "expression = \"ARC_CODE NOT IN (14, 28)\"\n",
    "\n",
    "arcpy.management.Append(source_fc, target, field_mapping_type, field_mapping, subtype, expression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you make a mistake and want to re-run the tool you can use Truncate Table to remove all features instead of making a new feature class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arcpy.management.TruncateTable(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all of the 'Caldera rim' lines into **GeologicLines**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_fc = gen_lines\n",
    "target = r\"C:\\_AAA\\mrp\\statemap\\nsa2gems\\Alaska_generalized.gdb\\GeologicMap\\GeologicLines\"\n",
    "field_mapping_type = \"NO_TEST\"\n",
    "field_mapping = '''\n",
    "LTYPE \"LTYPE\" true true false 200 Text 0 0,First,#,AKStategeolarc_generalized,LINE_TYPE,0,100\n",
    "'''\n",
    "subtype = \"\"\n",
    "expression = \"ARC_CODE IN (14, 28)\"\n",
    "\n",
    "arcpy.management.Append(source_fc, target, field_mapping_type, field_mapping, subtype, expression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the map unit polygons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_fc = gen_polys\n",
    "target = r\"C:\\_AAA\\mrp\\statemap\\nsa2gems\\Alaska_generalized.gdb\\GeologicMap\\MapUnitPolys\"\n",
    "field_mapping_type = \"NO_TEST\"\n",
    "field_mapping = '''\n",
    "MapUnit \"MapUnit\" true true false 10 Text 0 0,First,#,AKStategeolpoly_generalized,GROUP_LABEL,0,10;\n",
    "Label \"Label\" true true false 50 Text 0 0,First,#,AKStategeolpoly_generalized,GROUP_LABEL2,0,10;\n",
    "Symbol \"Symbol\" true true false 254 Text 0 0,First,#,AKStategeolpoly_generalized,GROUP_SYMBOL,-1,-1\n",
    "'''\n",
    "subtype = \"\"\n",
    "expression = \"\"\n",
    "\n",
    "arcpy.management.Append(source_fc, target, field_mapping_type, field_mapping, subtype, expression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arcpy.management.TruncateTable(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.0 Use Attribute By Key Values to parse LYTPE\n",
    "<a href=\"https://github.com/usgs/gems-tools-pro/wiki/GeMS_ToolsDocumentation#CreateNewDatabase\" target=\"_blank\">**Attribute By Key Values**</a> automates the process of selecting features based on a single-value attribute and then running field calculations on the appropriate GeMS fields in order to parse concatenated values. For example, the ```LTYPE``` value ```Normal fault, location certain``` might be translated to:\n",
    "```\n",
    "Type = \"normal fault”\n",
    "IsConcealed = \"N\"\n",
    "LocationConfidenceMeters = 200\n",
    "ExistenceConfidence = \"certain\"\n",
    "IdentifyConfidence = \"certain\"\n",
    "Symbol = \"01.01.03\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Build a key-value text file\n",
    "To use this tool, we have to first create a data dictionary that maps values between a single-value attribute and the appropriate GeMS fields. This is called the \"key value\" file and an example, **Dig24K_KeyValues.txt**, that you can start with or edit is in the **Resources** folder adjacent to the location of the toolbox file.\n",
    "\n",
    "Let's revisit the list of unique **LINE_TYPE** values we built in section 3.2. It would be nice write to that to a text file and separate some of the values by the pipe (```|```) delimiter to save some typing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-use the df variable as a reminder of how to go from an ArcGIS feature class to a pandas dataframe\n",
    "# although it is not a best-practice to repeatedly read in a data source, it's very inexpensive here\n",
    "arr = arcpy.da.TableToNumPyArray(gen_lines, ('LINE_TYPE'))\n",
    "df = pd.DataFrame(arr)\n",
    "\n",
    "# cast the LINE_TYPE column of values into a pandas Series object and get the unique values\n",
    "ltypes = pd.Series(df['LINE_TYPE']).unique()\n",
    "#ltypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that ```ltypes``` is a numpy array data type. Arrays are [computational advantageous](https://learnpython.com/blog/python-array-vs-list/) when the number of values is very large and for numerical calculations. This isn't the case for the string processing we have to do here and python lists are slightly easier to use. We could convert it using ```list(ltypes)```, but this is a good time to get rid of those pesky end-of-line characters so I will use a list comprehension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltypes = [line.strip() for line in ltypes]\n",
    "#for l in ltypes: print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the key_value.txt file we need to build, each feature class that needs values parsed from a \"key\" field e.g., ```LTYPE```, into dependent GeMS fields, has a section like this: \n",
    "```\n",
    "name of feature class\n",
    "original type field | GeMS field 1 | GeMS field 2 | GeMS field n\n",
    "type 1 | value 1 | value 2 | value n\n",
    "type 2 | value 1 | value 2 | value n\n",
    "etc.\n",
    "```\n",
    "\n",
    "Let's modify the list, adding all of the contents of this section, before we write the list to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through the line types and set some values\n",
    "# we'll review lcm later in map view, but for now use 50, 100, 200 \n",
    "# for certain, approximate, and inferred\n",
    "for i, val in enumerate(ltypes):\n",
    "    gems_type = val.split(',')[0].lower()\n",
    "    \n",
    "    if 'concealed' in val.lower():\n",
    "        is_concealed = 'y'\n",
    "    else:\n",
    "        is_concealed = 'n'\n",
    "    \n",
    "    if 'approx' in val.lower():\n",
    "        lcm = '100'\n",
    "    elif 'inferred' in val.lower():\n",
    "        lcm = '200'\n",
    "    else:\n",
    "        lcm = '50'\n",
    "    \n",
    "    if 'queried' in val.lower():\n",
    "        ex_conf = 'questionable'\n",
    "        id_conf = 'questionable'\n",
    "    else:\n",
    "        ex_conf = 'certain'\n",
    "        id_conf = 'certain'\n",
    "        \n",
    "    val_list = ' | '.join([val, gems_type, is_concealed, lcm, ex_conf, id_conf])\n",
    "    ltypes[i] = val_list\n",
    "\n",
    "# finally, add the header lines for this section\n",
    "# the name of the feature class and the list of dependent fields\n",
    "ltypes.insert(0,'ContactsAndFaults')\n",
    "field_header = ['LTYPE', 'Type', 'IsConcealed', 'LocationConfidenceMeters', 'ExistenceConfidence', 'IdentityConfidence']\n",
    "ltypes.insert(1, ' | '.join(field_header))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the list to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a path to the file\n",
    "key_val_file = r\"C:\\_AAA\\mrp\\statemap\\nsa2gems\\nsa2gems_key_values.txt\"\n",
    "\n",
    "# use open(file, 'a') if you are adding to a file\n",
    "# note that when you use with:, the file is closed after the indented code has run\n",
    "with open(key_val_file, 'w') as key_val:\n",
    "    key_val.write(\"\\n\".join(ltypes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, there will probably be a few odd Types to edit. Double-check in a text editor and clean things up. For example, creating a section for GeologicLines was easier to do manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the file\n",
    "with open(key_val_file, 'r') as key_val:\n",
    "    print(key_val.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2  Run the tool\n",
    "Now, set up variables like we did for the **CreateDatabase** tool and call the **Attribute By Key Values** tool. First, call the tool with no parameters to see the ```Usage``` string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "arcpy.AttributeByKeyValues_gems()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# key_val_file was set above when we created and edited the file\n",
    "key_val_file = r\"C:\\_AAA\\mrp\\statemap\\nsa2gems\\nsa2gems_key_values.txt\"\n",
    "gdb = r\"C:\\_AAA\\mrp\\statemap\\nsa2gems\\Alaska_generalized_1.gdb\"\n",
    "# call the tool\n",
    "arcpy.AttributeByKeyValues_gems(gdb, key_val_file, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.0 Calculate FGDC symbols for lines\n",
    "Based on the attributes we just calculated using the **Attribute By Key Values** tool and a display scale for the map data, we can calculate the appropriate FGDC symbol using the **Set Symbol Value** tool "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arcpy.SetSymbols_gems()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "Python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
