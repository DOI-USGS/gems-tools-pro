{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook for validating a GeMS database one rule at a time\n",
    "I use this for testing. You can programmatically write errors into the database on purpose to check that the tool finds them.\n",
    "\n",
    "Each section starts with copying a source database into a scratch workspace and then running the function(s) in the Validate Database tool for that rule and printing the results. This step is in the first code cell after the rule header. For testing, I then modify the database as necessary and then run the function(s) again, which I imagine most people using this notebook would not care to do.  \n",
    "\n",
    "Notes:\n",
    "1. add the folder paths and path to the database in the cell below by deleting the commented part of the line (starting with `#` after the `=` sign), browsing to the folder or database in the Catalog window, and click-and-dragging the item to the space after the `=` sign.\n",
    "2. `arcpy.management.Delete(gdb_c)` at the end of some sections is not always necessary. If the `copy` command at the beginning of a section fails, and there is no call to delete the database at the end, try adding it.\n",
    "3. most rule functions return a list, the first three items of which are used to build headers and anchors in the report htmls. Items beyond that will be the errors.\n",
    "4. if you edit any of the scripts imported (modules renamed as `vd`, `gdef`, `guf`, `alc`) while Pro is open, you need to reload them before you run the code cell again. After your edits, add the line `reload(vd)`, for example, to the top of the cell and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change these variables\n",
    "gdb = # path\\to\\geodatabase\\or\\geopackage\n",
    "scratch = # path\\to\\writable\\scratch\\space\n",
    "scripts = # path\\to\\scripts\\folder\\of\\toolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(scripts_folder)\n",
    "import GeMS_ValidateDatabase as vd\n",
    "import GeMS_Definition as gdef\n",
    "import GeMS_utilityFunctions as guf\n",
    "import GeMS_ALaCarte as alc\n",
    "from importlib import reload\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML is built into some function results for nice rendering in the validate reports\n",
    "# but we'll just remove html tags for display in this notebook\n",
    "CLEANR = re.compile('<.*?>') \n",
    "def clean(raw_html):\n",
    "  cleantext = re.sub(CLEANR, \"\", raw_html)\n",
    "  return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up some starting variables\n",
    "gdb_n = Path(gdb).name\n",
    "gdb_c = f\"{scratch}\\\\{gdb_n}\"\n",
    "if Path(gdb).suffix == \".gpkg\":\n",
    "    is_gpkg = True\n",
    "else:\n",
    "    is_gpkg = False\n",
    "    \n",
    "# path to the reference GeoMaterialDict file\n",
    "ref_gmd = scripts_folder /  \"GeoMaterialDict.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Rule 2.1 - Has required elements: nonspatial tables DataSources, DescriptionOfMapUnits, GeoMaterialDict; feature dataset GeologicMap with feature classes ContactsAndFaults and MapUnitPolys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# check the rule on the unmodified database\n",
    "arcpy.management.Copy(gdb, gdb_c)\n",
    "d = vd.guf.gdb_object_dict(gdb_c)\n",
    "errors, topology_pairs, sr_warnings = vd.rule2_1(d, is_gpkg)\n",
    "\n",
    "print(\"MISSING\")\n",
    "for err in errors[3:]: \n",
    "    print(f\"  {clean(err)}\")\n",
    "    \n",
    "print(\"TOPOLOGY PAIRS\")\n",
    "for tp in topology_pairs:\n",
    "    print(f\"  {tp}\") \n",
    "    \n",
    "print(\"SPATIAL REFERENCE WARNINGS\")\n",
    "if len(sr_warnings):\n",
    "    for warn in sr_warnings: print(f\"  {clean(warn)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# remove elements and check results\n",
    "for n in (\"DataSources\", \"DescriptionOfMapUnits\", \"GeoMaterialDict\", \"GeologicMap\", \"ContactsAndFaults\", \"MapUnitPolys\"):\n",
    "    if Path(gdb_c).exists:\n",
    "        arcpy.management.Delete(gdb_c)\n",
    "    arcpy.management.Copy(gdb, gdb_c)\n",
    "    db_dict = vd.guf.gdb_object_dict(gdb_c)\n",
    "    if n in db_dict:\n",
    "        arcpy.management.Delete(db_dict[n]['catalogPath'])\n",
    "        del db_dict[n]\n",
    "    errors = vd.rule2_1(db_dict, is_gpkg)[0]\n",
    "    print(\"MISSING\")\n",
    "    for err in errors[3:]: print(f\"  {clean(err)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The test below checks whether a \"topology pair\" can be found. A topology pair is any pair of similarly named `ContactsAndFaults` and `MapUnitPolys` feature classes. For example, if a feature class called `SurficialContactsAndFaults` is found, there should also be a `SurficialMapUnitPolys`. In the case of file geodatabases, the pairs need not be inside a feature dataset. There can be multiple topology pairs. For example, a single file gdb or geopackage could have a `Surficial` pair and a `Bedrock` pair, the requirement being that both feature classes have the same prefix and suffix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# change the name of a required element\n",
    "# if the name change only includes a suffix or prefix, the tool should still identify \n",
    "# the table as a GeMS object\n",
    "arcpy.management.Copy(gdb, gdb_c)\n",
    "d = guf.gdb_object_dict(gdb_c)\n",
    "\n",
    "caf = d[\"ContactsAndFaults\"][\"catalogPath\"]\n",
    "new_caf = f\"{caf}_2\"\n",
    "arcpy.management.Rename(caf, new_caf)\n",
    "d = vd.guf.gdb_object_dict(gdb_c)\n",
    "errors = vd.rule2_1(d, is_gpkg)[0]\n",
    "\n",
    "print(\"MISSING\")\n",
    "for err in errors[3:]: print(f\"  {clean(err)}\")\n",
    "\n",
    "arcpy.management.Delete(gdb_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Rule 2.2 - Required fields within required elements are present and correctly defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# check the rule on the unmodified database\n",
    "arcpy.management.Copy(gdb, gdb_c)\n",
    "d = vd.guf.gdb_object_dict(gdb_c)\n",
    "errors, schema_extensions, sr_warnings = vd.check_fields(d, 2, [])\n",
    "\n",
    "print(\"ERRORS\")\n",
    "for err in errors: print(f\"  {clean(err)}\")\n",
    "print(\"EXTENSIONS\")\n",
    "for ext in schema_extensions: print(f\"  {clean(ext)}\")          \n",
    "print(\"FIELD WARNINGS\")\n",
    "for warn in sr_warnings: print(f\"  {clean(warn)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# change the names of some fields, delete others\n",
    "change = {\"MapUnit\": \"mapunit\",\n",
    "          \"Type\": \"Type2\",\n",
    "          \"HierarchyKey\": \"HKEY\"}\n",
    "delete = (\"ExistenceConfidence\", \"Label\")\n",
    "\n",
    "arcpy.management.Copy(gdb, gdb_c)\n",
    "d = vd.guf.gdb_object_dict(gdb_c)\n",
    "for k,v in d.items():\n",
    "    table = v['catalogPath']\n",
    "    if \"fields\" in v:\n",
    "        flds = [f.name for f in v[\"fields\"]]\n",
    "        # can't use AlterField on geopackages\n",
    "        # test first.\n",
    "        if not is_gpkg:\n",
    "            for a in change:\n",
    "                if a in flds:\n",
    "                    arcpy.management.AlterField(table, a, change[a])\n",
    "                    \n",
    "        for n in delete:\n",
    "            if n in flds:\n",
    "                arcpy.management.DeleteField(table, n)\n",
    "                \n",
    "d = vd.guf.gdb_object_dict(gdb_c)\n",
    "errors, schema_extensions, warnings = vd.check_fields(d, 2, [])\n",
    "\n",
    "print(\"MISSING\")\n",
    "for err in errors[3:]: print(f\"  {clean(err)}\")\n",
    "print(\"EXTENSIONS\")\n",
    "for ext in schema_extensions: print(f\"  {clean(ext)}\")\n",
    "print(\"WARNINGS\")\n",
    "for warn in warnings: print(f\"  {clean(warn)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Rule 2.3 - GeologicMap topology: no internal gaps or overlaps in MapUnitPolys, boundaries of MapUnitPolys are covered by ContactsAndFaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# make a copy\n",
    "arcpy.management.Copy(gdb, gdb_c)\n",
    "# check for existing Topology.gdb\n",
    "t_path = Path(scratch) / \"Topology.gdb\"\n",
    "if t_path.exists:\n",
    "    arcpy.management.Delete(str(t_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# noodle around with the topology OR NOT IF YOU ARE CHECKING AN UNMODIFIED DATABASE\n",
    "# change names of topo pairs, etc.\n",
    "d = guf.gdb_object_dict(gdb_c)\n",
    "topo_pairs = vd.rule2_1(d, is_gpkg)[1]\n",
    "\n",
    "level2_results = vd.check_topology(d, scratch, False, topo_pairs)[0]\n",
    "\n",
    "print(\"TOPOLOGY ERRORS\")\n",
    "for err in level2_results[3:]: print(f\"  {clean(err)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Rule 2.4 - All map units in MapUnitPolys have entries in DescriptionOfMapUnits table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# check the rule on the unmodified database\n",
    "arcpy.management.Copy(gdb, gdb_c)\n",
    "d = vd.guf.gdb_object_dict(gdb_c)\n",
    "missing, all_map_units, fds_map_units = vd.check_map_units(d, 2, [], {})\n",
    "\n",
    "print(\"MISSING\")\n",
    "for miss in missing[3:]: print(f\"  {clean(missing)}\\n\")\n",
    "print(\"ALL LEVEL 2 MAP UNITS\")\n",
    "print(\", \".join(set(all_map_units)), \"\\n\")\n",
    "print(\"MAP UNITS IN DMU, EACH FEATURE DATASET\")\n",
    "for k,v in fds_map_units.items():\n",
    "    print(k) \n",
    "    print(\", \".join(v), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# add a MapUnit to MapUnitPolys that is not in the DMU\n",
    "mup = d[\"MapUnitPolys\"][\"catalogPath\"]\n",
    "with arcpy.da.UpdateCursor(mup, \"MapUnit\") as cursor:\n",
    "    for i,row in enumerate(cursor):\n",
    "        if i == 0:\n",
    "            row[0] = \"foo\"\n",
    "        if i == 1:\n",
    "            row[0] = \"bar\"\n",
    "        cursor.updateRow(row)\n",
    "        \n",
    "missing, all_map_units, fds_map_units = vd.check_map_units(d, 2, [], {})\n",
    "\n",
    "print(\"MISSING\")\n",
    "for miss in missing[3:]: print(f\"{clean(miss)}\\n\")\n",
    "print(\"ALL LEVEL 2 MAP UNITS\")\n",
    "print(\", \".join(set(all_map_units)), \"\\n\")\n",
    "print(\"MAP UNITS IN DMU, EACH FEATURE DATASET\")\n",
    "for k,v in fds_map_units.items():\n",
    "    print(k) \n",
    "    print(\", \".join(v), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Rule 2.5 - No duplicate MapUnit values in DescriptionOfMapUnit table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# check the rule on the unmodified database\n",
    "arcpy.management.Copy(gdb, gdb_c)\n",
    "d = vd.guf.gdb_object_dict(gdb_c)\n",
    "dmu = d[\"DescriptionOfMapUnits\"][\"catalogPath\"]\n",
    "dups = guf.get_duplicates(dmu, \"Mapunit\")\n",
    "\n",
    "print(\"DUPLICATES\")\n",
    "print(\", \".join(dups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# copy a MapUnit value in the DMU\n",
    "dmu = d[\"DescriptionOfMapUnits\"][\"catalogPath\"]\n",
    "with arcpy.da.UpdateCursor(dmu, \"MapUnit\", where_clause=\"MapUnit is not null\" ) as cursor:\n",
    "    for i, row in enumerate(cursor):\n",
    "        if i == 0:\n",
    "            mu = row[0]\n",
    "        if i == 1:\n",
    "            row[0] = mu\n",
    "        cursor.updateRow(row)\n",
    "        \n",
    "dups = guf.get_duplicates(dmu, \"Mapunit\")\n",
    "print(\"DUPLICATES\")\n",
    "print(\", \".join(dups))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Rule 2.6 - Certain field values within required elements have entries in Glossary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(vd)\n",
    "# check the rule on the unmodified database\n",
    "arcpy.management.Copy(gdb, gdb_c)\n",
    "d = vd.guf.gdb_object_dict(gdb_c)\n",
    "missing_glossary_terms, all_gloss_terms = vd.glossary_check(d, 2, [])\n",
    "\n",
    "print(\"MISSING\")\n",
    "for miss in missing_glossary_terms[3:]: print(f\"{clean(miss)}\")\n",
    "print(\"\\n\")\n",
    "print(\"ALL GLOSSARY TERMS\")\n",
    "print(\", \".join(all_gloss_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# required element can be renamed but gems_equivalent is correctly assigned\n",
    "# and the fields are still checked\n",
    "caf = d[\"ContactsAndFaults\"]['catalogPath']\n",
    "arcpy.management.Rename(caf, f\"{caf}_2\")\n",
    "d = guf.gdb_object_dict(gdb_c)\n",
    "\n",
    "# investigate 'gems_equivalent', un-comment the next two lines\n",
    "# for k,v in d.items():\n",
    "#     print(k, v[\"gems_equivalent\"])\n",
    "missing_glossary_terms, all_gloss_terms = vd.glossary_check(d, 2, [])\n",
    "\n",
    "print(\"MISSING\")\n",
    "for miss in missing_glossary_terms[3:]: print(f\"{clean(miss)}\")\n",
    "print(\"\\n\")\n",
    "print(\"ALL GLOSSARY TERMS\")\n",
    "print(\", \".join(all_gloss_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# reset a value in a required field\n",
    "arcpy.management.Copy(gdb, gdb_c)\n",
    "d = guf.gdb_object_dict(gdb_c)\n",
    "caf = d[\"ContactsAndFaults\"]['catalogPath']\n",
    "with arcpy.da.UpdateCursor(caf, \"Type\") as cursor:\n",
    "    for i,row in enumerate(cursor):\n",
    "        if i == 0:\n",
    "            row[0] = \"foobar\"\n",
    "            cursor.updateRow(row)\n",
    "missing_glossary_terms, all_gloss_terms = vd.glossary_check(d, 2, [])\n",
    "\n",
    "print(\"MISSING\")\n",
    "for miss in missing_glossary_terms[3:]: print(f\"{clean(miss)}\")\n",
    "print(\"\\n\")\n",
    "print(\"ALL GLOSSARY TERMS\")\n",
    "print(\", \".join(all_gloss_terms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Rule 2.7 - No duplicate Term values in Glossary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# check the rule on the unmodified database\n",
    "arcpy.management.Copy(gdb, gdb_c)\n",
    "d = vd.guf.gdb_object_dict(gdb_c)\n",
    "glo = d[\"Glossary\"][\"catalogPath\"]\n",
    "dups = guf.get_duplicates(glo, \"Term\")\n",
    "\n",
    "print(\"DUPLICATES\")\n",
    "print(\", \".join(dups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Copy one of the terms in Glossary\n",
    "arcpy.management.Copy(gdb, gdb_c)\n",
    "d = guf.gdb_object_dict(gdb_c)\n",
    "glo = d[\"Glossary\"]['catalogPath']\n",
    "with arcpy.da.UpdateCursor(glo, \"Term\") as cursor:\n",
    "    for i, row in enumerate(cursor):\n",
    "        if i == 0:\n",
    "            term = row[0]\n",
    "        if i == 1:\n",
    "            row[0] = term\n",
    "        cursor.updateRow(row)\n",
    "d = guf.gdb_object_dict(gdb_c)\n",
    "dups = guf.get_duplicates(glo, \"Term\")\n",
    "\n",
    "print(\"DUPLICATES\")\n",
    "print(\", \".join(dups))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Rule 2.8 - All SourceID values in required elements have entries in DataSources table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# check the rule on the unmodified database\n",
    "importlib.reload(vd)\n",
    "arcpy.management.Copy(gdb, gdb_c)\n",
    "d = vd.guf.gdb_object_dict(gdb_c)\n",
    "errors, all_sources = vd.sources_check(d, 2, [])\n",
    "\n",
    "print(\"ERRORS\")\n",
    "for err in errors[3:]: print(f\"  {clean(err)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# add a DataSourceID that is not in DataSources\n",
    "caf = d[\"ContactsAndFaults\"][\"catalogPath\"]\n",
    "with arcpy.da.UpdateCursor(caf, \"DataSourceID\") as cursor:\n",
    "    for i, row in enumerate(cursor):\n",
    "        if i == 0:\n",
    "            row[0] = \"foobar\"\n",
    "            cursor.updateRow(row)\n",
    "d = guf.gdb_object_dict(gdb_c)\n",
    "\n",
    "errors, all_sources = vd.sources_check(d, 2, [])\n",
    "\n",
    "print(\"ERRORS\")\n",
    "for err in errors[3:]: print(f\"  {clean(err)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Rule 2.9 - No duplicate DataSources_ID values in DataSources table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# check the rule on the unmodified database\n",
    "arcpy.management.Copy(gdb, gdb_c)\n",
    "d = vd.guf.gdb_object_dict(gdb_c)\n",
    "ds = d[\"DataSources\"][\"catalogPath\"]\n",
    "dups = guf.get_duplicates(ds, \"DataSources_ID\")\n",
    "\n",
    "print(\"DUPLICATES\")\n",
    "print(\", \".join(dups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# add a duplicate DataSource_ID\n",
    "with arcpy.da.UpdateCursor(ds, \"DataSources_ID\") as cursor:\n",
    "    for i, row in enumerate(cursor):\n",
    "        if i == 0:\n",
    "            val = row[0]\n",
    "        if i == 1:\n",
    "            row[0] = val\n",
    "        cursor.updateRow(row)\n",
    "\n",
    "duplicates = guf.get_duplicates(ds, \"DataSources_ID\")\n",
    "\n",
    "print(\"DUPLICATES\")\n",
    "print(\", \".join(dups))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Rule 3.1 - Table and field definitions conform to GeMS schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# check the rule on the unmodified database\n",
    "importlib.reload(vd)\n",
    "arcpy.management.Copy(gdb, gdb_c)\n",
    "d = vd.guf.gdb_object_dict(gdb_c)\n",
    "errors, schema_extensions, warnings = vd.check_fields(d, 3, [])\n",
    "\n",
    "print(\"MISSING\")\n",
    "for err in errors[3:]: print(f\"  {clean(err)}\")\n",
    "print(\"EXTENSIONS\")\n",
    "for ext in schema_extensions: print(f\"  {clean(ext)}\")\n",
    "print(\"WARNINGS\")\n",
    "for warn in warnings: print(f\"  {clean(warn)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# add an optional GeMS-defined feature class\n",
    "if not is_gpkg:\n",
    "    fd = \"GeologicMap\"\n",
    "    sr = d[\"GeologicMap\"][\"spatialReference\"].name   \n",
    "    fc = \"OverlayPolys\"\n",
    "else:\n",
    "    fd = \"#\"\n",
    "    sr = d[\"MapUnitPolys\"][\"spatialReference\"].name\n",
    "    fc = \"OverlayPolys\"\n",
    "\n",
    "vt = arcpy.ValueTable(3)\n",
    "vt.addRow(f\"{fd} {sr} {fc}\")\n",
    "alc.process(gdb_c, vt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# delete required fields from this optional feature class\n",
    "d = guf.gdb_object_dict(gdb_c)\n",
    "fc = \"OverlayPolys\"\n",
    "delete_fields = [\"Type\", \"Label\"]\n",
    "for f in delete_fields:\n",
    "    arcpy.management.DeleteField(d[fc]['catalogPath'], f)\n",
    "d = guf.gdb_object_dict(gdb_c)\n",
    "\n",
    "errors = vd.check_fields(d, 3, [])[0]\n",
    "\n",
    "print(\"MISSING\")\n",
    "for err in errors[3:]: print(f\"  {clean(err)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# add a required field but with the wrong length, and type. Again, we're not checking for nullable fields\n",
    "arcpy.management.Copy(gdb, gdb_c)\n",
    "d = guf.gdb_object_dict(gdb_c)\n",
    "if not is_gpkg:\n",
    "    fd = \"GeologicMap\"\n",
    "    sr = d[\"GeologicMap\"][\"spatialReference\"].name   \n",
    "    fc = \"OverlayPolys\"\n",
    "else:\n",
    "    fd = \"#\"\n",
    "    sr = d[\"MapUnitPolys\"][\"spatialReference\"].name\n",
    "    fc = \"OverlayPolys\"\n",
    "    \n",
    "vt = arcpy.ValueTable(3)\n",
    "vt.addRow(f\"{fd} {sr} {fc}\")\n",
    "alc.process(gdb_c, vt)\n",
    "d = guf.gdb_object_dict(gdb_c)\n",
    "arcpy.management.DeleteField(d[\"OverlayPolys\"]['catalogPath'], \"Label\")\n",
    "# set length and f_type separately\n",
    "# length only considered if type is text\n",
    "length = 25\n",
    "f_type = \"float\"  # \"text\"\n",
    "arcpy.management.AddField(d[\"OverlayPolys\"]['catalogPath'], \"Label\", f_type, field_length=length)\n",
    "d = guf.gdb_object_dict(gdb_c)\n",
    "\n",
    "errors = vd.check_fields(d, 3, [])[0]\n",
    "\n",
    "print(\"MISSING\")\n",
    "for err in errors[3:]: print(f\"  {clean(err)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Rule 3.2 - All map-like feature datasets obey topology rules. No MapUnitPolys gaps or overlaps. No ContactsAndFaults overlaps, self-overlaps, or self-intersections. MapUnitPoly boundaries covered by ContactsAndFaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# make a copy\n",
    "arcpy.management.Copy(gdb, gdb_c)\n",
    "# check for existing Topology.gdb\n",
    "t_path = Path(scratch) / \"Topology.gdb\"\n",
    "if t_path.exists:\n",
    "    arcpy.management.Delete(str(t_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# noodle around with the topology OR NOT IF YOU ARE CHECKING AN UNMODIFIED DATABASE\n",
    "# change names of topo pairs, etc.\n",
    "d = guf.gdb_object_dict(gdb_c)\n",
    "topo_pairs = vd.rule2_1(d, is_gpkg)[1]\n",
    "level3_results = vd.check_topology(d, scratch, False, topo_pairs)[1]\n",
    "\n",
    "print(\"TOPOLOGY ERRORS\")\n",
    "for err in level3_results[3:]: print(f\"  {clean(err)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Rule 3.3 - No missing required values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# check the rule on the unmodified database\n",
    "arcpy.management.Copy(gdb, gdb_c)\n",
    "d = vd.guf.gdb_object_dict(gdb_c)\n",
    "\n",
    "errors, warnings = vd.rule3_3(d)\n",
    "\n",
    "print(\"MISSING VALUES\")\n",
    "for err in errors[3:]: print(f\"  {clean(err)}\")\n",
    "    \n",
    "print(\"MISSING WARNINGS\")\n",
    "for warn in warnings[1:]: print(f\"  {clean(warn)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# delete a couple values from a NoNulls field\n",
    "# and a couple values from a non-critical NoNulls field\n",
    "# eg, FieldID in Stations in which nulls maybe shouldn't \n",
    "# exist but won't break compliancy if they do\n",
    "mup = d[\"MapUnitPolys\"][\"catalogPath\"]\n",
    "with arcpy.da.UpdateCursor(mup, \"MapUnit\") as cursor:\n",
    "    for i, row in enumerate(cursor):\n",
    "        if i == 0:\n",
    "            row[0] = None\n",
    "        if i == 1:\n",
    "            row[0] = None\n",
    "        cursor.updateRow(row)\n",
    "\n",
    "sta = d[\"Stations\"][\"catalogPath\"]\n",
    "with arcpy.da.UpdateCursor(sta, \"FieldID\") as cursor:\n",
    "    for i, row in enumerate(cursor):\n",
    "        if i == 0:\n",
    "            row[0] = None\n",
    "        if i == 1:\n",
    "            row[0] = None\n",
    "        cursor.updateRow(row)\n",
    "        \n",
    "errors, warnings = vd.rule3_3(d)\n",
    "\n",
    "print(\"MISSING VALUES\")\n",
    "for err in errors[3:]: print(f\"  {clean(err)}\")\n",
    "    \n",
    "print(\"MISSING WARNINGS\")\n",
    "for warn in warnings[1:]: print(f\"  {clean(warn)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Rule 3.4 - No missing terms in Glossary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`defined_term_fields_list = (\n",
    "    \"Type\",\n",
    "    \"ExistenceConfidence\",\n",
    "    \"IdentityConfidence\",\n",
    "    \"ParagraphStyle\",\n",
    "    \"GeoMaterialConfidence\",\n",
    "    \"ErrorMeasure\",\n",
    "    \"AgeUnits\",\n",
    "    \"LocationMethod\",\n",
    "    \"ScientificConfidence\",\n",
    ")`\n",
    "\n",
    "Values in `defined_term_fields` fields not found in Glossary are errors. Values in non-defined fields that end in `type`, `confidence`, or `method` that are not found in Glossary are warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(vd)\n",
    "# check the rule on the unmodified database\n",
    "arcpy.management.Copy(gdb, gdb_c)\n",
    "d = vd.guf.gdb_object_dict(gdb_c)\n",
    "\n",
    "# first get all glossary terms from a level 2 check\n",
    "missing_terms, all_gloss_terms = vd.glossary_check(d, 2, [])\n",
    "\n",
    "# and then use that in a level 3 check\n",
    "missing_terms, all_gloss_terms, warnings = vd.glossary_check(d, 3, all_gloss_terms)\n",
    "\n",
    "print(\"MISSING GLOSSARY TERMS\")\n",
    "for miss in missing_terms[3:]: print(f\"  {clean(miss)}\")\n",
    "print(\"WARNINGS\")\n",
    "for warn in warnings: print(f\"{clean(warn)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# add some terms not found in the glossary\n",
    "# first, add a gems-like field. Values here should get reported as warnings\n",
    "cart = d[\"CartographicLines\"][\"catalogPath\"]\n",
    "arcpy.management.AddField(cart, 'CartoMethod', 'TEXT')\n",
    "d = vd.guf.gdb_object_dict(gdb_c)\n",
    "\n",
    "# add terms to required field in a non-core table and a gems-like field that is not in the glossary\n",
    "with arcpy.da.UpdateCursor(cart, [\"Type\", \"CartoMethod\"]) as cursor:\n",
    "    for i, row in enumerate(cursor):\n",
    "        if i == 0:\n",
    "            row[0] = \"foobar\"\n",
    "            row[1] = \"found it on a map\"\n",
    "        cursor.updateRow(row)\n",
    "\n",
    "# first get all glossary terms from a level 2 check\n",
    "missing_terms, all_gloss_terms = vd.glossary_check(d, 2, [])\n",
    "\n",
    "# and then use that in a level 3 check\n",
    "missing_terms, all_gloss_terms, warnings = vd.glossary_check(d, 3, all_gloss_terms)\n",
    "\n",
    "print(\"MISSING TERMS\")\n",
    "for miss in missing_terms[3:]: print(f\"  {clean(miss)}\")\n",
    "print(\"WARNINGS\")\n",
    "for warn in warnings: \n",
    "    warn = warn.replace(\"\\n\", \"\")\n",
    "    warn = \" \".join(warn.split())\n",
    "    print(f\"  {clean(warn)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Rule 3.5 - No unnecessary terms in Glossary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# check the rule on the unmodified database\n",
    "arcpy.management.Copy(gdb, gdb_c)\n",
    "d = vd.guf.gdb_object_dict(gdb_c)\n",
    "\n",
    "# need to run glossary_check at levels 2 and 3 to get all_gloss_terms\n",
    "# first get all glossary terms from a level 2 check\n",
    "missing_terms, all_gloss_terms = vd.glossary_check(d, 2, [])\n",
    "\n",
    "# and then use that in a level 3 check\n",
    "missing_terms, all_gloss_terms, warnings = vd.glossary_check(d, 3, all_gloss_terms)\n",
    "\n",
    "unused = vd.rule3_5_and_7(d, \"glossary\", all_gloss_terms)\n",
    "print(\"UNUSED TERMS\")\n",
    "for term in unused[3:]: print(f\" {clean(term)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# add a value to the Glossary that is not used anywhere\n",
    "gloss = d[\"Glossary\"][\"catalogPath\"]\n",
    "with arcpy.da.UpdateCursor(gloss, \"Term\") as cursor:\n",
    "    for i, row in enumerate(cursor):\n",
    "        if i == 0:\n",
    "            row[0] = \"foobar\"\n",
    "        cursor.updateRow(row)\n",
    "\n",
    "# need to run glossary_check at levels 2 and 3 to get all_gloss_terms\n",
    "# first get all glossary terms from a level 2 check\n",
    "missing_terms, all_gloss_terms = vd.glossary_check(d, 2, [])\n",
    "\n",
    "# and then use that in a level 3 check\n",
    "missing_terms, all_gloss_terms, warnings = vd.glossary_check(d, 3, all_gloss_terms)\n",
    "\n",
    "unused = vd.rule3_5_and_7(d, \"glossary\", all_gloss_terms)\n",
    "print(\"UNUSED TERMS\")\n",
    "for term in unused[3:]: print(f\" {clean(term)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Rule 3.6 - No missing sources in DataSources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# check the rule on the unmodified database\n",
    "arcpy.management.Copy(gdb, gdb_c)\n",
    "d = vd.guf.gdb_object_dict(gdb_c)\n",
    "\n",
    "# first run sources_check at level 2 to collect all_sources from required core elements\n",
    "all_sources = []\n",
    "missing_ids, all_sources = vd.sources_check(d, 2, all_sources)\n",
    "\n",
    "# then, run at level 3 to check the rest\n",
    "missing_ids, all_sources = vd.sources_check(d, 3, all_sources)\n",
    "\n",
    "print(\"MISSING DATASOURCES\")\n",
    "for miss in missing_ids[3:]: print(f\"  {clean(miss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# add a data source to a non-core GeMS table that is not in DataSources\n",
    "sta = d[\"Stations\"][\"catalogPath\"]\n",
    "with arcpy.da.UpdateCursor(sta, \"DataSourceID\") as cursor:\n",
    "    for i, row in enumerate(cursor):\n",
    "        if i == 0:\n",
    "            row[0] = \"DASfoobar\"\n",
    "        # delete a DataSourceID\n",
    "        if i == 1:\n",
    "            row[0] = None\n",
    "        cursor.updateRow(row)\n",
    "        \n",
    "# first run sources_check at level 2 to collect all_sources from required core elements\n",
    "all_sources = []\n",
    "missing_ids, all_sources = vd.sources_check(d, 2, all_sources)\n",
    "\n",
    "# then, run at level 3 to check the rest\n",
    "missing_ids, all_sources = vd.sources_check(d, 3, all_sources)\n",
    "\n",
    "print(\"MISSING DATASOURCES\")\n",
    "for miss in missing_ids[3:]: print(f\"  {clean(miss)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Rule 3.7 - No unnecessary sources in DataSources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# check the rule on the unmodified database\n",
    "arcpy.management.Copy(gdb, gdb_c)\n",
    "d = vd.guf.gdb_object_dict(gdb_c)\n",
    "\n",
    "# need to run sources_check at levels 2 and 3 to get all_sources\n",
    "# first get all sources from a level 2 check\n",
    "all_sources = []\n",
    "missing_ids, all_sources = vd.sources_check(d, 2, all_sources)\n",
    "\n",
    "# then, run at level 3 to check the rest\n",
    "missing_ids, all_sources = vd.sources_check(d, 3, all_sources)\n",
    "\n",
    "# then check rule3_5_and_7\n",
    "unused = vd.rule3_5_and_7(d, \"datasources\", all_sources)\n",
    "\n",
    "print(\"UNUSED TERMS\")\n",
    "for ds in unused[3:]: print(f\" {clean(ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# add a value to the Glossary that is not used anywhere\n",
    "ds = d[\"DataSources\"][\"catalogPath\"]\n",
    "with arcpy.da.UpdateCursor(ds, \"DataSources_ID\") as cursor:\n",
    "    for i, row in enumerate(cursor):\n",
    "        if i == 0:\n",
    "            row[0] = \"DASfoobar\"\n",
    "        cursor.updateRow(row)\n",
    "\n",
    "# need to run sources_check at levels 2 and 3 to get all_sources\n",
    "# first get all sources from a level 2 check\n",
    "all_sources = []\n",
    "missing_ids, all_sources = vd.sources_check(d, 2, all_sources)\n",
    "\n",
    "# then, run at level 3 to check the rest\n",
    "missing_ids, all_sources = vd.sources_check(d, 3, all_sources)\n",
    "\n",
    "# then check rule3_5_and_7\n",
    "unused = vd.rule3_5_and_7(d, \"datasources\", all_sources)\n",
    "\n",
    "print(\"UNUSED TERMS\")\n",
    "for ds in unused[3:]: print(f\" {clean(ds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Rule 3.8 - No map units without entries in DescriptionOfMapUnits and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# check the rule on the unmodified database\n",
    "arcpy.management.Copy(gdb, gdb_c)\n",
    "d = vd.guf.gdb_object_dict(gdb_c)\n",
    "\n",
    "# first, run check_map_units at level 2 which collects map units from MapUnitPolys\n",
    "all_map_units = []\n",
    "fds_map_units = {}\n",
    "msgs, all_map_units, fds_map_units = vd.check_map_units(d, 2, all_map_units, fds_map_units)\n",
    "\n",
    "# and then at level 3 to extend all_map_units with units from all tables with 'MapUnit'\n",
    "msgs3_8, msgs3_9, all_map_units, fds_map_units, mu_warnings = vd.check_map_units(d, 3, all_map_units, fds_map_units)\n",
    "\n",
    "print(\"MISSING MAPUNITS\")\n",
    "for mu in msgs3_8[3:]:\n",
    "    mu = \" \".join(mu.split())\n",
    "    print(f\"  {clean(mu)}\")\n",
    "    \n",
    "print(\"WARNINGS\")\n",
    "for warn in mu_warnings: \n",
    "    warn = \" \".join(warn.split())\n",
    "    print(f\"  {clean(warn)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# first, run check_map_units at level 2 which collects map units from MapUnitPolys\n",
    "all_map_units = []\n",
    "fds_map_units = {}\n",
    "msgs, all_map_units, fds_map_units = vd.check_map_units(d, 2, all_map_units, fds_map_units)\n",
    "\n",
    "# add a random map unit to a non-core element\n",
    "clines = d[\"Stations\"][\"catalogPath\"]\n",
    "with arcpy.da.UpdateCursor(clines, [\"MapUnit\", \"ObservedMapunit\"]) as cursor:\n",
    "    for i, row in enumerate(cursor):\n",
    "        if i == 0:\n",
    "            row[0] = \"foo\"\n",
    "        if i == 1:\n",
    "            row[1] = \"bar\"\n",
    "        cursor.updateRow(row)\n",
    "\n",
    "# run again at level 3 to extend all_map_units with units from all tables with 'MapUnit'\n",
    "msgs3_8, msgs3_9, all_map_units, fds_map_units, mu_warnings = vd.check_map_units(d, 3, all_map_units, fds_map_units)\n",
    "\n",
    "print(\"MISSING MAPUNITS\")\n",
    "for mu in msgs3_8[3:]:\n",
    "    mu = \" \".join(mu.split())\n",
    "    print(f\"  {clean(mu)}\")\n",
    "    \n",
    "print(\"WARNINGS\")\n",
    "for warn in mu_warnings: \n",
    "    warn = \" \".join(warn.split())\n",
    "    print(f\"  {clean(warn)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Rule 3.9 - No unnecessary MapUnits in DescriptionOfMapUnits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# check the rule on the unmodified database\n",
    "arcpy.management.Copy(gdb, gdb_c)\n",
    "d = vd.guf.gdb_object_dict(gdb_c)\n",
    "\n",
    "msgs3_8, msgs3_9, all_map_units, fds_map_units, mu_warnings = vd.check_map_units(d, 3, all_map_units, fds_map_units)\n",
    "\n",
    "print(\"MISSING MAPUNITS\")\n",
    "for mu in msgs3_9[3:]:\n",
    "    mu = \" \".join(mu.split())\n",
    "    print(f\"  {clean(mu)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# add an extra map unit to DescriptionOfMapUnits\n",
    "dmu = d[\"DescriptionOfMapUnits\"][\"catalogPath\"]\n",
    "with arcpy.da.UpdateCursor(dmu, \"MapUnit\") as cursor:\n",
    "    for i, row in enumerate(cursor):\n",
    "        if i == 0:\n",
    "            row[0] = \"foobar\"\n",
    "        cursor.updateRow(row)\n",
    "        \n",
    "msgs3_8, msgs3_9, all_map_units, fds_map_units, mu_warnings = vd.check_map_units(d, 3, all_map_units, fds_map_units)\n",
    "\n",
    "print(\"UNUSED MAPUNITS\")\n",
    "for mu in msgs3_9[3:]:\n",
    "    mu = \" \".join(mu.split())\n",
    "    print(f\"  {clean(mu)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Rule 3.10 - HierarchyKey values in DescriptionOfMapUnits are unique and well formed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# check the rule on the unmodified database\n",
    "arcpy.management.Copy(gdb, gdb_c)\n",
    "d = vd.guf.gdb_object_dict(gdb_c)\n",
    "hkey_errors, hkey_warnings = vd.rule3_10(d)\n",
    "\n",
    "print(\"HKEY ERRORS\")\n",
    "for err in hkey_errors[3:]: print(f\"  {clean(err)}\")\n",
    "\n",
    "print(\"HKEY WARNINGS\")\n",
    "for warn in hkey_warnings[1:]: print(f\"  {clean(warn)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# take a look at the HierarchyKeys\n",
    "dmu = d[\"DescriptionOfMapUnits\"][\"catalogPath\"]\n",
    "hkeys = [r[0] for r in arcpy.da.SearchCursor(dmu, \"HierarchyKey\")]\n",
    "hkeys.sort()\n",
    "for hkey in hkeys: print(hkey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# add a weird HierarchyKey\n",
    "dmu = d[\"DescriptionOfMapUnits\"][\"catalogPath\"]\n",
    "with arcpy.da.UpdateCursor(dmu, \"HierarchyKey\") as cursor:\n",
    "    for i, row in enumerate(cursor):\n",
    "        if i == 0:\n",
    "            row[0] = \"1/2\"\n",
    "        cursor.updateRow(row)\n",
    "\n",
    "hkey_errors, hkey_warnings = vd.rule3_10(d)\n",
    "\n",
    "print(\"HKEY ERRORS\")\n",
    "for err in hkey_errors[3:]: print(f\"  {clean(err)}\")\n",
    "\n",
    "print(\"HKEY WARNINGS\")\n",
    "for warn in hkey_warnings[1:]: print(f\"  {clean(warn)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Rule 3.11 - All values of GeoMaterial are defined in GeoMaterialDict. GeoMaterialDict is as specified in the GeMS standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# check the rule on the unmodified database\n",
    "arcpy.management.Delete(gdb_c)\n",
    "importlib.reload(vd)\n",
    "arcpy.management.Copy(gdb, gdb_c)\n",
    "d = vd.guf.gdb_object_dict(gdb_c)\n",
    "\n",
    "errors = vd.rule3_11(d, str(ref_gmd))\n",
    "print(\"GEOMATERIAL ERRORS\")\n",
    "for err in errors[3:]: print(f\"  {clean(err)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# delete a geomaterial and a definition from GeoMaterialDict\n",
    "# finding a null value in GeoMaterialDict causes the rule function \n",
    "# to return early so no other checks are made. Skip this cell\n",
    "# to get to the other checks\n",
    "gmd = d[\"GeoMaterialDict\"][\"catalogPath\"]\n",
    "with arcpy.da.UpdateCursor(gmd, [\"GeoMaterial\", \"Definition\"]) as cursor:\n",
    "    for i, row in enumerate(cursor):\n",
    "        if i == 0:\n",
    "            row[0] = None\n",
    "        if i == 1:\n",
    "            row[1] = None\n",
    "        cursor.updateRow(row)\n",
    "        \n",
    "errors = vd.rule3_11(d, str(ref_gmd))\n",
    "print(\"GEOMATERIAL ERRORS\")\n",
    "for err in errors[3:]: print(f\"  {clean(err)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# change a geomaterial and a definition in GeoMaterial\n",
    "gmd = d[\"GeoMaterialDict\"][\"catalogPath\"]\n",
    "with arcpy.da.UpdateCursor(gmd, [\"GeoMaterial\", \"Definition\"]) as cursor:\n",
    "    for i, row in enumerate(cursor):\n",
    "        if i == 0:\n",
    "            row[0] = \"choss\"\n",
    "        if i == 1:\n",
    "            row[1] = \"I believe this is some kind of rock\"\n",
    "        cursor.updateRow(row)\n",
    "        \n",
    "errors = vd.rule3_11(d, str(ref_gmd))\n",
    "print(\"GEOMATERIAL ERRORS\")\n",
    "for err in errors[3:]: print(f\"  {clean(err)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# add a weird geomaterial to DMU\n",
    "dmu = d[\"DescriptionOfMapUnits\"][\"catalogPath\"]\n",
    "with arcpy.da.UpdateCursor(dmu, \"GeoMaterial\") as cursor:\n",
    "    for i, row in enumerate(cursor):\n",
    "        if i == 0:\n",
    "            row[0] = \"choss\"\n",
    "        cursor.updateRow(row)\n",
    "        \n",
    "errors = vd.rule3_11(d, str(ref_gmd))\n",
    "print(\"GEOMATERIAL ERRORS\")\n",
    "for err in errors[3:]: print(f\"  {clean(err)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Rule 3.12 - No duplicate \\_ID values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# check the rule on the unmodified database\n",
    "arcpy.management.Copy(gdb, gdb_c)\n",
    "d = vd.guf.gdb_object_dict(gdb_c)\n",
    "dups = vd.rule3_12(d)\n",
    "\n",
    "print(\"DUPLICATES\")\n",
    "for dup in dups[3:]: print(f\"  {clean(dup)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# duplicate an _ID value\n",
    "importlib.reload(vd)\n",
    "caf = d[\"ContactsAndFaults\"][\"catalogPath\"]\n",
    "with arcpy.da.UpdateCursor(caf, \"ContactsAndFaults_ID\") as cursor:\n",
    "    for i, row in enumerate(cursor):\n",
    "        if i == 0:\n",
    "            val = row[0]\n",
    "        if i == 1:\n",
    "            row[0] = val\n",
    "        cursor.updateRow(row)\n",
    "\n",
    "dups = vd.rule3_12(d)\n",
    "\n",
    "print(\"DUPLICATES\")\n",
    "for dup in dups[3:]: print(f\"  {clean(dup)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Rule 3.13 - No zero-length or whitespace-only strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# check the rule on the unmodified database\n",
    "arcpy.management.Copy(gdb, gdb_c)\n",
    "d = vd.guf.gdb_object_dict(gdb_c)\n",
    "\n",
    "zero_length_strings, leading_trailing_spaces = vd.rule3_13(d)\n",
    "\n",
    "print(\"ZERO LENGTH STRINGS\")\n",
    "for zero in zero_length_strings[3:]: print(f\"  {clean(zero)}\")\n",
    "    \n",
    "print(\"LEAD/TRAILING WHITESPACE\")\n",
    "for lead in leading_trailing_spaces[1:]: print(f\"  {clean(lead)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# add some bad null values\n",
    "caf = d[\"ContactsAndFaults\"][\"catalogPath\"]\n",
    "with arcpy.da.UpdateCursor(caf, \"Type\") as cursor:\n",
    "    for i, row in enumerate(cursor):\n",
    "        if i == 0:\n",
    "            row[0] = \"\"\n",
    "        if i == 1:\n",
    "            row[0] = \" \"\n",
    "        if i == 2:\n",
    "            row[0] = \"<NULL>\"\n",
    "        cursor.updateRow(row)\n",
    "\n",
    "results = vd.rule3_13(d)\n",
    "zero_length_strings, leading_trailing_spaces = vd.rule3_13(d)\n",
    "\n",
    "print(\"ZERO LENGTH STRINGS\")\n",
    "for zero in zero_length_strings[3:]: print(f\"  {clean(zero)}\")\n",
    "    \n",
    "print(\"LEAD/TRAILING WHITESPACE\")\n",
    "for lead in leading_trailing_spaces[1:]: print(f\"  {clean(lead)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for editor tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### List extra tables and fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# make a copy\n",
    "arcpy.management.Copy(gdb, gdb_c)\n",
    "d = vd.guf.gdb_object_dict(gdb_c)\n",
    "\n",
    "# collect the extra fields as logged by check_fields for each level\n",
    "extra_fields = []\n",
    "errors, extra_fields, fld_warnings = vd.check_fields(d, 2, extra_fields)\n",
    "errors, extra_fields, fld_warnings = vd.check_fields(d, 3, extra_fields)\n",
    "all_extras = vd.extra_tables(d, extra_fields)\n",
    "\n",
    "for extra in all_extras:\n",
    "    print(clean(extra))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "Python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
